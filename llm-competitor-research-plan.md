# Competitor Research Copilot: Spec, Evaluation, and Delivery Plan

## Assumptions
- Product teams need faster, evidence-backed competitor intelligence for weekly planning and launch decisions.
- Initial scope is English-language, publicly available sources only (websites, blogs, press releases, filings, job postings).
- First release is internal-only for research, product marketing, and strategy users.
- No direct integration with paid third-party data providers in v1.

## Open Questions
- Which competitors and geographies must be in v1 scope?
- What is the required freshness SLA (near-real-time vs daily batch)?
- Are there legal/compliance constraints on storing quoted content beyond metadata and short snippets?
- Which output channels are required at launch (web app only vs Slack/Notion/Email)?

## 1. Product Spec

### 1.1 Summary
- Goal: Deliver an LLM assistant that tracks competitors and produces reliable, cited summaries, comparisons, and alerts.
- Business impact: Reduce manual research time and improve decision quality for roadmap and GTM planning.
- Business success metrics:
  - 40% reduction in analyst time per weekly brief by end of quarter.
  - 60% weekly active usage among target internal teams.
  - 25% increase in stakeholder confidence score for competitor updates.

### 1.2 Problem and Users
- Target users:
  - Product managers
  - Product marketing managers
  - Competitive intelligence analysts
  - Leadership stakeholders
- Primary use cases:
  - Generate weekly competitor briefs.
  - Answer ad hoc questions with evidence.
  - Compare product capabilities across named competitors.
  - Alert users when significant competitor events occur.
- Non-goals:
  - Financial forecasting or investment advice.
  - Monitoring private/internal competitor data.
  - Fully automated strategic recommendations without human review.

### 1.3 Scope
- In scope:
  - Public-source ingestion and indexing.
  - Conversational Q&A with citations.
  - Scheduled brief generation.
  - Event/change alerts from tracked entities.
- Out of scope:
  - Non-English multilingual support in v1.
  - Proprietary paid data connectors.
  - Autonomous outbound actions (e.g., posting externally).

### 1.4 Inputs and Outputs
- Input sources and formats:
  - RSS feeds, web pages, release notes, SEC filings, and selected social posts via approved APIs.
  - User prompts and configurable watchlists.
- Output format and constraints:
  - Structured Markdown brief with sections: Summary, Key Changes, Evidence, Competitive Implications, Confidence.
  - Every factual claim must include at least one source citation with timestamp.
- Required grounding:
  - Answers must be grounded only in retrieved source passages.
  - If evidence is insufficient, system should explicitly state uncertainty.

### 1.5 Functional Requirements
| ID | Requirement | Priority | Rationale | Acceptance metric |
| --- | --- | --- | --- | --- |
| FR-1 | Ingest and index approved public sources for each competitor at least daily. | P0 | Freshness for monitoring value. | 99% of scheduled ingestion jobs complete daily; failed jobs retried within 2h. |
| FR-2 | Generate weekly briefs per competitor and portfolio. | P0 | Core analyst workflow. | 95% of scheduled briefs generated by configured time. |
| FR-3 | Provide answer-level citations for every factual statement. | P0 | Trust and auditability. | >=98% factual statements include valid citation links. |
| FR-4 | Support ad hoc Q&A over indexed corpus with source-grounded responses. | P0 | Fast exploration and follow-ups. | Offline grounded QA score >=0.80 on holdout set. |
| FR-5 | Produce side-by-side comparison tables for selected competitors and features. | P1 | Common strategy task. | >=90% of generated tables pass human factual review. |
| FR-6 | Detect and alert on major events (pricing, launch, leadership, partnerships). | P1 | Timely actionability. | Event detection recall >=0.85 and precision >=0.80. |
| FR-7 | Let users configure watchlists, topics, and alert thresholds. | P1 | Personalization and relevance. | >=90% of saved configs successfully applied to next run. |
| FR-8 | De-duplicate repeated news and cluster similar events. | P1 | Noise reduction. | Duplicate alert rate <10% weekly. |
| FR-9 | Export briefs to Slack and downloadable Markdown/PDF. | P2 | Workflow integration. | >=95% export jobs succeed. |
| FR-10 | Show confidence and uncertainty when evidence is weak/conflicting. | P0 | Prevent overclaiming. | 100% low-evidence answers include uncertainty notice. |
| FR-11 | Maintain admin controls for source allowlist/blocklist. | P1 | Data quality governance. | Admin updates propagate to ingestion within 30 minutes. |
| FR-12 | Maintain audit logs for prompt, retrieval context IDs, and outputs. | P0 | Compliance and debugging. | 100% production requests logged with trace ID. |

### 1.6 Non-Functional Requirements
- Latency:
  - Interactive Q&A p95 <= 12s end-to-end.
  - Brief generation p95 <= 3 minutes for portfolio run.
- Cost/token budget:
  - Average Q&A inference cost <= $0.08/request.
  - Weekly total inference + retrieval cost <= budget threshold set by ops.
- Reliability/uptime:
  - 99.5% monthly uptime for user-facing API.
- Privacy/compliance:
  - Do not store credentials or private data in prompts.
  - Redact detected PII in logs.
- Localization/language:
  - English-only output in v1.

### 1.7 Data and Knowledge
- Data sources and ownership:
  - Public web sources under approved legal policy.
  - Internal ownership: Competitive Intelligence + Platform Engineering.
- Freshness and update cadence:
  - Daily crawl baseline, hourly for high-priority sources.
- PII/sensitive data handling:
  - Avoid ingestion of user-private content.
  - Hash user IDs in analytics and apply retention limits.
- Licensing/usage rights:
  - Store metadata and short excerpts; avoid full-content redistribution unless licensed.

### 1.8 Model and Architecture
- Baseline approach:
  - RAG pipeline with hybrid retrieval (BM25 + embedding) and reranking.
- Tools/fine-tuning plan:
  - Start with prompt + retrieval only; consider fine-tuning after 2 release cycles if error classes persist.
- Prompting strategy:
  - Structured system prompt enforcing citation and uncertainty behavior.
- Tooling/integrations:
  - Vector store, crawl scheduler, alert queue, Slack webhook integration.

### 1.9 UX and Safety
- Response style and tone:
  - Neutral, concise, evidence-first.
- Refusal/safety rules:
  - Refuse unsupported claims and speculative allegations.
  - Mark unverified claims as unconfirmed.
- Hallucination prevention:
  - Retrieval-required mode for factual questions.
  - Citation validation before response emission.

### 1.10 Risks and Mitigations
| Risk | Impact | Likelihood | Mitigation |
| --- | --- | --- | --- |
| Source quality drift | High | Medium | Source scoring, periodic allowlist review, automatic low-quality source demotion. |
| Hallucinated synthesis | High | Medium | Citation-required decoding, abstain policy, targeted eval gates. |
| Legal/licensing exposure | High | Low-Med | Legal-approved ingestion policy, snippet length limits, source attribution. |
| Alert fatigue | Medium | Medium | Clustering, threshold tuning, user-level preference controls. |
| Cost spikes on heavy usage | Medium | Medium | Token caps, caching, model routing by complexity. |

### 1.11 Success Metrics and Guardrails
- Model quality:
  - Grounded answer score >=0.80.
  - Citation validity >=98%.
- Safety/policy:
  - Unsupported-claim rate <=2%.
  - PII leakage incidents: 0 tolerated in production.
- Guardrails:
  - If citation validity <95% in any 24h window, auto-disable proactive alert summaries.

### 1.12 Dependencies and Milestones
- Dependencies:
  - Source ingestion service, vector index infra, auth/access controls, analytics stack.
- Milestones:
  - M1 (Week 2): Source ingestion MVP and schema.
  - M2 (Week 4): RAG Q&A alpha with citations.
  - M3 (Week 6): Brief generation + alerting beta.
  - M4 (Week 8): Production rollout with monitoring and runbooks.

### 1.13 Dependencies on Evaluation and Delivery
- Evaluation dependency: FR-1 to FR-12 require explicit offline and online tests before release.
- Delivery dependency: production launch requires guardrails, on-call ownership, rollback automation, and policy checks.

## 2. Evaluation Plan

### 2.1 Evaluation Objectives
- Decision supported:
  - Whether to release to internal users and then expand org-wide.
- Primary success criteria:
  - Meets go/no-go thresholds for grounding, citation validity, safety, latency, and cost.

### 2.2 Test Sets
- Offline sources:
  - Historic competitor news/events, release notes, pricing page changes, leadership announcements.
- Size/coverage targets:
  - 1,200 QA items across 12 competitors.
  - 300 comparison tasks.
  - 200 alert-detection event windows.
- Labeling guidelines:
  - Gold answers require cited supporting evidence and confidence rating.
  - Mark answer as fail if key claim lacks valid citation.
- Split strategy:
  - 70/15/15 split for development, calibration, and blind holdout.

### 2.3 Metrics
#### Automatic metrics
- Grounded QA correctness (LLM-as-judge + human-verified sample): >=0.80.
- Citation validity: >=98%.
- Retrieval metrics: recall@10 >=0.90 for answerable queries; MRR >=0.75.
- Latency: p95 <=12s for interactive Q&A.
- Cost: mean <=$0.08 per Q&A request.

#### Human evaluation
- Rubric (1-5): factual accuracy, completeness, actionability, clarity, and calibration.
- Pass criteria:
  - Mean score >=4.2/5 on factual accuracy and >=4.0/5 overall.
- Rater calibration:
  - 3 raters per sample subset; weighted kappa >=0.7.

### 2.4 Baselines and Comparisons
- Baselines:
  - Existing manual analyst workflow (time/quality benchmark).
  - Prompt-only (no retrieval) model baseline.
- Improvement targets:
  - >=20% relative improvement in grounded correctness vs prompt-only baseline.
  - >=40% time reduction vs manual brief preparation.

### 2.5 Coverage Matrix
| Requirement ID | Test cases | Metric | Pass threshold |
| --- | --- | --- | --- |
| FR-1 | 120 ingestion runs | Job completion rate | >=99% daily completion |
| FR-2 | 150 scheduled briefs | On-time completion | >=95% |
| FR-3 | 600 factual statements | Citation validity | >=98% |
| FR-4 | 1,200 QA items | Grounded correctness | >=0.80 |
| FR-5 | 300 comparison tasks | Human factual pass rate | >=90% |
| FR-6 | 200 event windows | Precision/Recall | P>=0.80, R>=0.85 |
| FR-7 | 100 config scenarios | Config application success | >=90% |
| FR-8 | 200 duplicate-prone events | Duplicate alert rate | <10% |
| FR-9 | 120 exports | Export success rate | >=95% |
| FR-10 | 180 low-evidence prompts | Uncertainty compliance | 100% |
| FR-11 | 60 admin updates | Propagation latency | <=30 min |
| FR-12 | 1,000 prod-like requests | Log completeness | 100% traceable |

### 2.6 Protocol
- Offline:
  - Run nightly eval pipeline on holdout sets.
  - Publish dashboard by metric family (quality, safety, latency, cost).
- Online:
  - A/B test with 20% pilot cohort vs current manual process.
- Statistical criteria:
  - 95% confidence intervals must clear go/no-go bars for primary metrics.

### 2.7 Quality Bars (Go/No-Go)
- Must-pass:
  - Citation validity >=98%.
  - Grounded correctness >=0.80.
  - Unsupported-claim rate <=2%.
  - p95 latency <=12s.
  - 0 critical privacy incidents.
- Nice-to-have:
  - Human overall score >=4.2/5.
  - Duplicate alert rate <=8%.

### 2.8 Error Analysis
- Failure modes to track:
  - Missing citations, stale evidence, incorrect synthesis across sources, noisy alerts.
- Sampling strategy:
  - Daily stratified sample by competitor, use case, and confidence band.

### 2.9 Safety and Risk Evaluation
- Toxicity/bias:
  - Evaluate narrative framing and loaded language in generated summaries.
- Privacy leakage:
  - Red-team prompt suite for hidden/system data extraction.
- Prompt injection/jailbreak:
  - Web content injection tests; enforce content sanitization and instruction hierarchy.

### 2.10 Reporting
- Reporting cadence:
  - Daily internal eval snapshot; weekly release-readiness review.
- Decision owner:
  - Product lead + applied AI lead + compliance delegate.

## 3. Delivery Plan

### 3.1 Release Scope
- Included:
  - Source ingestion, RAG Q&A, weekly briefs, event alerts, citations, audit logs, Slack export.
- Deferred:
  - Multilingual support, paid-data connectors, automated strategic recommendations.

### 3.2 Deployment Plan
- Environments:
  - Dev -> Staging -> Production with promotion gates.
- Infrastructure/model hosting:
  - Managed LLM endpoint, dedicated retrieval service, queue-based ingestion workers.
- Rollout steps:
  - Stage 1: internal dogfood (CI + PMM).
  - Stage 2: pilot org release with feature flag.
  - Stage 3: full internal rollout on stable metrics.

### 3.3 Rollout Strategy
- Phased rollout:
  - 10% -> 30% -> 100% over 2-3 weeks.
- Feature flags:
  - Separate flags for Q&A, briefs, and alerts.
- Rollback criteria:
  - Trigger rollback if any must-pass metric fails for 2 consecutive monitoring windows.

### 3.4 Monitoring and Alerts
- Product metrics:
  - WAU/DAU, brief completion rate, alert engagement.
- Model quality:
  - Citation validity trend, grounded correctness sample audit.
- Safety:
  - Unsupported claims, policy violations, incident counts.
- Latency/cost:
  - p95 latency, per-request spend, daily burn vs budget.

### 3.5 Guardrails
- Policy filters:
  - Unsafe content and unsupported allegation checks.
- Rate limits/quotas:
  - Per-user request quotas and burst limits.
- Fallback behavior:
  - If retrieval fails, return "insufficient evidence" and no synthesized claim.

### 3.6 Ops and Ownership
- On-call:
  - Platform engineering on-call for infra; applied AI owner for model quality incidents.
- Escalation path:
  - Severity-based routing to product, security, and legal when required.
- Runbooks:
  - Incident triage, rollback, source disablement, and evaluation rerun procedures.

### 3.7 Documentation and Enablement
- Internal docs:
  - Source governance, prompt policy, eval dashboard interpretation.
- User-facing docs:
  - How to configure watchlists, interpret confidence, and validate citations.
- Training:
  - 1-hour onboarding for pilot users.

### 3.8 Feedback Loop
- Collection:
  - In-product thumbs up/down with reason codes and free text.
- Cadence:
  - Weekly quality review and monthly model/routing update cycle.
- Update plan:
  - Prioritize fixes by severity and frequency; update prompts/retrieval monthly or sooner if quality bars regress.

## 4. Final Assumptions and Open Questions

### Assumptions
- Users accept citation-linked summaries as draft intelligence, not final truth.
- Legal approves selected source ingestion policy for v1.
- Pilot cohort can provide weekly human ratings for calibration.

### Open Questions
- Which compliance framework applies (SOC2-only vs additional regional/legal controls)?
- Is near-real-time alerting needed for a subset of sources?
- What are hard budget limits per month and per active user?
- Should leadership-facing briefs use a different style or stricter confidence threshold?
